using BenchmarkTools, GPUCellListSPH, CUDA, StaticArrays


points = map(x->tuple(x...), eachrow(rand(Float64, 200000, 2)))

#cpupoints = map(x->SVector(tuple(x...)), eachrow(rand(Float64, 200000, 2)))
cellsize = (0.04, 0.04, 0.04)
dist = 0.04

dx  = 0.02
h   = 1.2 * sqrt(2) * dx
H   = 2h
hâ»Â¹ = 1/h
Hâ»Â¹ = 1/H
dist = H
cellsize = (dist, dist, dist)

points = cpupoints 
if length(points) < 3 error("wrong dimention") end

N = length(first(points))                                          # Number of points 
pcell = CUDA.fill((Int32(0), Int32(0), Int32(0)), N)                  # list of cellst for each particle


MIN    = minimum.(points)                                    # minimal value 
MIN    = @. MIN - abs((MIN + sqrt(eps())) * sqrt(eps()))     # minimal value  (a lillte bit less for better cell fitting)
MAX    = maximum.(points)                                    # maximum                           
range  = MAX .- MIN                                          # range

CELL   = @. ceil(Int, range/cellsize)                        # number of cells 

cellpnum     = CUDA.zeros(Int32, CELL...)                    # array for number of particles in each cell 
cnt          = CUDA.zeros(Int32, 1)                          # temp array for particles counter (need to count place for each pair in pair list)
gpupoints       = CuArray{eltype(first(points))}.(points)                                   # array with particles / points

GPUCellListSPH.cellmap!(pcell, cellpnum, gpupoints,  cellsize, MIN)                 # modify pcell, cellpnum < pcell - map each point to cell, cellpnum - number of particle in each cell

maxpoint = Int(ceil(maximum(cellpnum)*1.05 + 1))                                # mppcell - maximum particle in cell for cell list (with reserve ~ 5%)
mppcell = maxpoint 
    
    
celllist     = CUDA.zeros(Int32, mppcell, CELL...)  

fill!(cellpnum, Int32(0))                                                          # set cell counter to zero 
GPUCellListSPH.fillcells_naive!(celllist, cellpnum,  pcell)  

maxneigh = maximum(cellpnum)*9


mpairs = GPUCellListSPH.Ğ¼axpairs(cellpnum)                                                 # mpairs - maximum pairs in pair list (all combination inside cell and neighboring cells (4))

    
    pairs    = CUDA.fill((zero(Int32), zero(Int32)), mpairs)   
    cnt          = CUDA.zeros(Int32, 1)                     # pair list
    GPUCellListSPH.neib_search!(pairs, cnt, cellpnum, gpupoints, celllist, dist)  











using GPUCellListSPH
using CSV, DataFrames, CUDA, BenchmarkTools
using SPHKernels, WriteVTK
path         = dirname(@__FILE__)
fluid_csv    = joinpath(path, "../test/input/FluidPoints_Dp0.02.csv")
boundary_csv = joinpath(path, "../test/input/BoundaryPoints_Dp0.02.csv")
DF_POINTS = append!(CSV.File(fluid_csv) |> DataFrame, CSV.File(boundary_csv) |> DataFrame)
cpupoints = tuple(eachcol(DF_POINTS[!, ["Points:0", "Points:2"]])...)
#cpupoints = tuple(eachcol(Float32.(DF_POINTS[!, ["Points:0", "Points:2"]]))...)
dx  = 0.02
h   = 1.2 * sqrt(2) * dx
H   = 2h
hâ»Â¹ = 1/h
Hâ»Â¹ = 1/H
dist = 1.1H
Ïâ‚€  = 1000.0
mâ‚€  = Ïâ‚€ * dx * dx
Î±   = 0.01
g   = 9.81
câ‚€  = sqrt(g * 2) * 20
Î³   = 7
Î”t  = dt  = 1e-5
Î´áµ©  = 0.1
CFL = 0.2
cellsize = (dist, dist)
sphkernel    = WendlandC2(Float64, 2)
system  = GPUCellList(cpupoints, cellsize, dist)
N       = system.n
Ï       = CUDA.zeros(Float64, N)
copyto!(Ï, DF_POINTS.Rhop)
ptype   = CUDA.zeros(Int32, N)
copyto!(ptype, DF_POINTS.ptype)
v       = CUDA.fill((0.0, 0.0), system.n)
sphprob =  SPHProblem(system, dx, h, H, sphkernel, Ï, v, ptype, Ïâ‚€, mâ‚€, Î”t, Î±, g, câ‚€, Î³, Î´áµ©, CFL)


    GPUCellListSPH.sphâˆ‘âˆ‡W!(sphprob.âˆ‘âˆ‡W, sphprob.âˆ‡W, sphprob.system.pairs,  sphprob.system.points, sphprob.sphkernel, sphprob.Hâ»Â¹)

    # kernels for each pair
    GPUCellListSPH.sphW!(sphprob.W, sphprob.system.pairs, sphprob.system.points, sphprob.Hâ»Â¹, sphprob.sphkernel)
    # kernels gradientfor each pair
    GPUCellListSPH.sphâˆ‡W!(sphprob.âˆ‡W,sphprob.system.pairs, sphprob.system.points, sphprob.Hâ»Â¹, sphprob.sphkernel)
    # density derivative with density diffusion
    GPUCellListSPH.âˆ‚Ïâˆ‚tDDT!(sphprob.âˆ‘âˆ‚Ïâˆ‚t,view(sphprob.system.pairs, 1:sphprob.system.pairsn), sphprob.âˆ‡W, sphprob.Ï, sphprob.v, sphprob.system.points, sphprob.h, sphprob.mâ‚€, sphprob.Ïâ‚€, sphprob.câ‚€, sphprob.Î³, sphprob.g, sphprob.Î´áµ©, sphprob.ptype; minthreads = 256) 
    #  pressure
    GPUCellListSPH.pressure!(sphprob.P, sphprob.Ï, sphprob.câ‚€, sphprob.Î³, sphprob.Ïâ‚€, sphprob.ptype) 
    # momentum equation 
    âˆ‚vâˆ‚t!(sphprob.âˆ‘âˆ‚vâˆ‚t,  sphprob.âˆ‡W, sphprob.P, view(sphprob.system.pairs, 1:sphprob.system.pairsn),  sphprob.mâ‚€, sphprob.Ï, sphprob.ptype) 
    # add artificial viscosity
    âˆ‚vâˆ‚t_av!(sphprob.âˆ‘âˆ‚vâˆ‚t, sphprob.âˆ‡W, view(sphprob.system.pairs, 1:sphprob.system.pairsn),  sphprob.system.points, sphprob.h, sphprob.Ï, sphprob.Î±, sphprob.v, sphprob.câ‚€, sphprob.mâ‚€, sphprob.ptype)
    # laminar shear stresse
    if sphprob.ğœˆ > 0
        âˆ‚vâˆ‚t_visc!(sphprob.âˆ‘âˆ‚vâˆ‚t, sphprob.âˆ‡W, sphprob.v, sphprob.Ï, sphprob.system.points, view(sphprob.system.pairs, 1:sphprob.system.pairsn), sphprob.h, sphprob.mâ‚€, sphprob.ğœˆ, sphprob.ptype)
    end
    # add gravity 
    âˆ‚vâˆ‚t_addgrav!(sphprob.âˆ‘âˆ‚vâˆ‚t, GPUCellListSPH.gravvec(sphprob.g, sphprob.dim)) 
    #  Boundary forces
    fbmolforce!(sphprob.âˆ‘âˆ‚vâˆ‚t, view(sphprob.system.pairs, 1:sphprob.system.pairsn), sphprob.system.points, 0.4, 2 * sphprob.dx, sphprob.ptype)


    # following steps (update_Ï!, update_vpâˆ‚vâˆ‚tÎ”t!, update_xpvÎ”t!) can be done in one kernel 
        # calc Ï at Î”tÂ½
        GPUCellListSPH.update_Ïpâˆ‚Ïâˆ‚tÎ”t!(sphprob.ÏÎ”tÂ½, sphprob.âˆ‘âˆ‚Ïâˆ‚t, sphprob.Î”t * 0.5, sphprob.Ïâ‚€, sphprob.ptype)
        # calc v at Î”tÂ½
        GPUCellListSPH.update_vpâˆ‚vâˆ‚tÎ”t!(sphprob.vÎ”tÂ½, sphprob.âˆ‘âˆ‚vâˆ‚t, sphprob.Î”t * 0.5, sphprob.ptype) 
        # calc x at Î”tÂ½
        GPUCellListSPH.update_xpvÎ”t!(sphprob.xÎ”tÂ½, sphprob.vÎ”tÂ½, sphprob.Î”t * 0.5)

        fill!(sphprob.âˆ‘âˆ‚vâˆ‚t[1], zero(T))
        fill!(sphprob.âˆ‘âˆ‚vâˆ‚t[2], zero(T))
        fill!(sphprob.âˆ‘âˆ‚vâˆ‚t[3], zero(T))

        GPUCellListSPH.Î”t_stepping(sphprob.buf, sphprob.âˆ‘âˆ‚vâˆ‚t, sphprob.v, sphprob.system.points, sphprob.câ‚€, sphprob.h, sphprob.CFL, (0,1))


    GPUCellListSPH.âˆ‚Ïâˆ‚tDDT!(âˆ‘âˆ‚Ïâˆ‚t, âˆ‡W, system.pairs, system.points, h, mâ‚€, Î´áµ©, câ‚€, Î³, g, Ïâ‚€, Ï, v, ptype) 
    
    GPUCellListSPH.âˆ‚Î âˆ‚t!(âˆ‘âˆ‚Î âˆ‚t, âˆ‡W, system.pairs, system.points, h, Ï, Î±, v, câ‚€, mâ‚€) 


    GPUCellListSPH.âˆ‚Ïâˆ‚tDDT!(sphprob.âˆ‘âˆ‚Ïâˆ‚t,view(sphprob.system.pairs, 1:sphprob.system.pairsn), sphprob.âˆ‡W, sphprob.Ï, sphprob.v, sphprob.system.points, sphprob.h, sphprob.mâ‚€, sphprob.Ïâ‚€, sphprob.câ‚€, sphprob.Î³, sphprob.g, sphprob.Î´áµ©, sphprob.ptype; minthreads = 256)  

    @benchmark  GPUCellListSPH.âˆ‚Ïâˆ‚tDDT!($sphprob.âˆ‘âˆ‚Ïâˆ‚t,$view(sphprob.system.pairs, 1:sphprob.system.pairsn), $sphprob.âˆ‡W, $sphprob.Ï, $sphprob.v, $sphprob.system.points, $sphprob.h, $sphprob.mâ‚€, $sphprob.Ïâ‚€, $sphprob.câ‚€, $sphprob.Î³, $sphprob.g, $sphprob.Î´áµ©, $sphprob.ptype; minthreads = 256) 
    # 256 - 136.300 / 148.200 / 171.975 Î¼s Â±  45.326 Î¼s

    @benchmark GPUCellListSPH.âˆ‚Î âˆ‚t!($âˆ‘âˆ‚Î âˆ‚t, $âˆ‡W, $system.pairs, $system.points, $h, $Ï, $Î±, $v, $câ‚€, $mâ‚€; minthreads = 1024) 

    @benchmark GPUCellListSPH.âˆ‚Ïâˆ‚tDDT_2!($âˆ‘âˆ‚Ïâˆ‚t, $system2.nlist, $system2.cnt, $system2.points, $sphkernel, $h, $Hâ»Â¹, $mâ‚€, $Î´áµ©, $câ‚€, $Î³, $g, $Ïâ‚€, $Ï, $v, $isboundary) 

#== ==#

prob= sphprob

Ï = copy(prob.Ï)
GPUCellListSPH.cspmcorr!(prob.buf2, prob.W, Ï , prob.mâ‚€, view(prob.system.pairs, 1:prob.system.pairsn), prob.ptype)


#=
function âˆ‚Ïâˆ‚tDDT3!(âˆ‘âˆ‚Ïâˆ‚t, buff, âˆ‡W, h, mâ‚€, Ïâ‚€, câ‚€, Î³, g, Î´áµ©, ptype; minthreads::Int = 1024)  where T
    Î·Â²    = (0.1*h)*(0.1*h)
    Î³â»Â¹   = 1/Î³
    DDTkh = 2 * h * Î´áµ© * câ‚€
    Cb    = (câ‚€ * câ‚€ * Ïâ‚€) * Î³â»Â¹
    DDTgz = Ïâ‚€ * g / Cb

    gpukernel = @cuda launch=false kernel_âˆ‚Ïâˆ‚tDDT3!(âˆ‘âˆ‚Ïâˆ‚t, buff, âˆ‡W,  Î·Â², mâ‚€, Ïâ‚€, Î³, Î³â»Â¹, DDTkh, DDTgz, ptype) 
    config = launch_configuration(gpukernel.fun)
    Nx = length(âˆ‡W)
    maxThreads = config.threads
    Tx  = min(minthreads, maxThreads, Nx)
    Bx  = cld(Nx, Tx)
    CUDA.@sync gpukernel(âˆ‘âˆ‚Ïâˆ‚t, buff, âˆ‡W,  Î·Â², mâ‚€, Ïâ‚€, Î³, Î³â»Â¹, DDTkh, DDTgz, ptype; threads = Tx, blocks = Bx)
end
function kernel_âˆ‚Ïâˆ‚tDDT3!(âˆ‘âˆ‚Ïâˆ‚t, buff, âˆ‡W,  Î·Â², mâ‚€, Ïâ‚€, Î³, Î³â»Â¹, DDTkh, DDTgz, ptype) 
    index = (blockIdx().x - Int32(1)) * blockDim().x + threadIdx().x
    if index <= length(âˆ‡W)
            Î”v    = buff[2][index]
            âˆ‘âˆ‚Ïâˆ‚t[1][index] = mâ‚€ * (Î”v[1] * âˆ‡W[index][1] + Î”v[2] * âˆ‡W[index][2])  #  Î”v â‹… âˆ‡Wáµ¢â±¼
    end
    return nothing
end

function âˆ‚Ïâˆ‚tDDT4!(âˆ‘âˆ‚Ïâˆ‚t, buff, âˆ‡W, h, mâ‚€, Ïâ‚€, câ‚€, Î³, g, Î´áµ©, ptype; minthreads::Int = 1024)  where T
    Î·Â²    = (0.1*h)*(0.1*h)
    Î³â»Â¹   = 1/Î³
    DDTkh = 2 * h * Î´áµ© * câ‚€
    Cb    = (câ‚€ * câ‚€ * Ïâ‚€) * Î³â»Â¹
    DDTgz = Ïâ‚€ * g / Cb

    gpukernel = @cuda launch=false kernel_âˆ‚Ïâˆ‚tDDT4!(âˆ‘âˆ‚Ïâˆ‚t, buff, âˆ‡W,  Î·Â², mâ‚€, Ïâ‚€, Î³, Î³â»Â¹, DDTkh, DDTgz, ptype) 
    config = launch_configuration(gpukernel.fun)
    Nx = length(âˆ‡W)
    maxThreads = config.threads
    Tx  = min(minthreads, maxThreads, Nx)
    Bx  = cld(Nx, Tx)
    CUDA.@sync gpukernel(âˆ‘âˆ‚Ïâˆ‚t, buff, âˆ‡W,  Î·Â², mâ‚€, Ïâ‚€, Î³, Î³â»Â¹, DDTkh, DDTgz, ptype; threads = Tx, blocks = Bx)
end
function kernel_âˆ‚Ïâˆ‚tDDT4!(âˆ‘âˆ‚Ïâˆ‚t, buff, âˆ‡W,  Î·Â², mâ‚€, Ïâ‚€, Î³, Î³â»Â¹, DDTkh, DDTgz, ptype) 
    index = (blockIdx().x - Int32(1)) * blockDim().x + threadIdx().x

    if index <= length(âˆ‡W)
            Î”x    = buff[1][index]
            rÂ²    = Î”x[1]^2 + Î”x[2]^2 
            Ïáµ¢    = buff[3][index]
            Ïâ±¼    = buff[4][index]
            dot3  = -(Î”x[1] * âˆ‡W[index][1] + Î”x[2] * âˆ‡W[index][2]) #  - Î”x â‹… âˆ‡Wáµ¢â±¼
            #if ptype[páµ¢] >= 1
                drhopvp = Ïâ‚€ * powfancy7th(1 + DDTgz * Î”x[2], Î³â»Â¹, Î³) - Ïâ‚€ 
                visc_densi = DDTkh  * (Ïâ±¼ - Ïáµ¢ - drhopvp) / (rÂ² + Î·Â²)
            #end
            âˆ‘âˆ‚Ïâˆ‚t[2][index] = visc_densi * dot3 * mâ‚€ / Ïâ±¼
    end
    return nothing
end
function âˆ‚Ïâˆ‚tDDT5!(âˆ‘âˆ‚Ïâˆ‚t, buff, âˆ‡W, h, mâ‚€, Ïâ‚€, câ‚€, Î³, g, Î´áµ©, ptype; minthreads::Int = 1024)  where T
    Î·Â²    = (0.1*h)*(0.1*h)
    Î³â»Â¹   = 1/Î³
    DDTkh = 2 * h * Î´áµ© * câ‚€
    Cb    = (câ‚€ * câ‚€ * Ïâ‚€) * Î³â»Â¹
    DDTgz = Ïâ‚€ * g / Cb

    gpukernel = @cuda launch=false kernel_âˆ‚Ïâˆ‚tDDT5!(âˆ‘âˆ‚Ïâˆ‚t, buff, âˆ‡W,  Î·Â², mâ‚€, Ïâ‚€, Î³, Î³â»Â¹, DDTkh, DDTgz, ptype) 
    config = launch_configuration(gpukernel.fun)
    Nx = length(âˆ‡W)
    maxThreads = config.threads
    Tx  = min(minthreads, maxThreads, Nx)
    Bx  = cld(Nx, Tx)
    CUDA.@sync gpukernel(âˆ‘âˆ‚Ïâˆ‚t, buff, âˆ‡W,  Î·Â², mâ‚€, Ïâ‚€, Î³, Î³â»Â¹, DDTkh, DDTgz, ptype; threads = Tx, blocks = Bx)
end
function kernel_âˆ‚Ïâˆ‚tDDT5!(âˆ‘âˆ‚Ïâˆ‚t, buff, âˆ‡W,  Î·Â², mâ‚€, Ïâ‚€, Î³, Î³â»Â¹, DDTkh, DDTgz, ptype) 
    index = (blockIdx().x - Int32(1)) * blockDim().x + threadIdx().x
    if index <= length(âˆ‡W)
            Î”x    = buff[1][index]
            rÂ²    = Î”x[1]^2 + Î”x[2]^2 
            Ïáµ¢    = buff[3][index]
            Ïâ±¼    = buff[4][index]
            dot3  = -(Î”x[1] * âˆ‡W[index][1] + Î”x[2] * âˆ‡W[index][2]) #  - Î”x â‹… âˆ‡Wáµ¢â±¼
            #if ptype[pâ±¼] >= 1
                drhopvn = Ïâ‚€ * powfancy7th(1 - DDTgz * Î”x[2], Î³â»Â¹, Î³) - Ïâ‚€
                visc_densi = DDTkh  * (Ïáµ¢ - Ïâ±¼ - drhopvn) / (rÂ² + Î·Â²)
            #end
            âˆ‘âˆ‚Ïâˆ‚t[3][index] = visc_densi * dot3 * mâ‚€ / Ïáµ¢
    end
    return nothing
end


function collect_âˆ‚Ïâˆ‚tDDT2!(âˆ‘âˆ‚Ïâˆ‚t::CuArray{T}, buff, pairs, ptype; minthreads::Int = 1024)  where T
    fill!(âˆ‘âˆ‚Ïâˆ‚t, zero(T))
    gpukernel = @cuda launch=false kernel_collect_âˆ‚Ïâˆ‚tDDT2!(âˆ‘âˆ‚Ïâˆ‚t, buff, pairs, ptype) 
    config = launch_configuration(gpukernel.fun)
    Nx = length(pairs)
    maxThreads = config.threads
    Tx  = min(minthreads, maxThreads, Nx)
    Bx  = cld(Nx, Tx)
    CUDA.@sync gpukernel(âˆ‘âˆ‚Ïâˆ‚t, buff, pairs, ptype; threads = Tx, blocks = Bx)
end
function kernel_collect_âˆ‚Ïâˆ‚tDDT2!(âˆ‘âˆ‚Ïâˆ‚t, buff, pairs, ptype) 
    tindex = (blockIdx().x - Int32(1)) * blockDim().x + threadIdx().x
    stride = gridDim().x * blockDim().x
    index      = tindex
    while index <= length(pairs)
        pair  = pairs[index]
        páµ¢    = pair[1]; pâ±¼ = pair[2]
        if ptype[páµ¢] > 0 && ptype[pâ±¼] > 0
            CUDA.@atomic âˆ‘âˆ‚Ïâˆ‚t[páµ¢] += buff[1][index] + buff[2][index]
            CUDA.@atomic âˆ‘âˆ‚Ïâˆ‚t[pâ±¼] += buff[1][index] + buff[3][index]
        end
        index += stride
    end
    return nothing
end
=#



function pairs_calk_test2!(buff) 
    gpukernel = @cuda launch=false kernel_pairs_calk_test2!(buff) 
    config = launch_configuration(gpukernel.fun)
    Nx = length(buff)
    maxThreads = config.threads
    Tx  = min( maxThreads, Nx)
    Bx  = cld(Nx, Tx)
    CUDA.@sync gpukernel(buff; threads = Tx, blocks = Bx)
end
function kernel_pairs_calk_test2!(buff) 
    index = (blockIdx().x - Int32(1)) * blockDim().x + threadIdx().x
    if index <= length(buff)
        pair = buff[index]
        a = pair.Ïáµ¢ + pair.Ïâ±¼ + pair.Î”x[1] + pair.Î”v[1]
    end
    return nothing
end
function pairs_calk_test1!(buff) 
    gpukernel = @cuda launch=false kernel_pairs_calk_test1!(buff) 
    config = launch_configuration(gpukernel.fun)
    Nx = length(buff)
    maxThreads = config.threads
    Tx  = min(maxThreads, Nx)
    Bx  = cld(Nx, Tx)
    CUDA.@sync gpukernel(buff; threads = Tx, blocks = Bx)
end
function kernel_pairs_calk_test1!(buff) 
    index = (blockIdx().x - Int32(1)) * blockDim().x + threadIdx().x
    if index <= length(buff)
        Î”x = buff[1][index] 
        Î”v = buff[2][index] 
        Ïáµ¢ = buff[3][index] 
        Ïâ±¼ = buff[4][index] 
        a = Ïáµ¢ + Ïâ±¼ + Î”x[1] + Î”v[2]
    end
    return nothing
end


@benchmark pairs_calk_test1!($buff) 

@benchmark pairs_calk_test2!($buffpp) 






















function sph_simulation(system, sphkernel, Ï, ÏÎ”tÂ½, v, vÎ”tÂ½, xÎ”tÂ½, âˆ‘âˆ‚Î âˆ‚t, âˆ‘âˆ‚Ïâˆ‚t, âˆ‘âˆ‚vâˆ‚t, sumW, sumâˆ‡W, âˆ‡Wâ‚™, Î”t, Ïâ‚€, isboundary, ml, h, Hâ»Â¹, mâ‚€, Î´áµ©, câ‚€, Î³, g, Î±; simn = 1)

    for iter = 1:simn
    GPUCellListSPH.update!(system)
    x     = system.points
    pairs = system.pairs

    fill!(sumW, zero(Float64))
    fill!(âˆ‘âˆ‚Ïâˆ‚t, zero(Float64))
    fill!(âˆ‘âˆ‚Î âˆ‚t, zero(Float64))
    fill!(âˆ‘âˆ‚vâˆ‚t, zero(Float64))

    if length(âˆ‡Wâ‚™) != length(pairs)
        CUDA.unsafe_free!(âˆ‡Wâ‚™)
        âˆ‡Wâ‚™ =  CUDA.fill((zero(Float64), zero(Float64)), length(system.pairs))
    end

    GPUCellListSPH.âˆ‘W_2d!(sumW, pairs, sphkernel, Hâ»Â¹)

    GPUCellListSPH.âˆ‘âˆ‡W_2d!(sumâˆ‡W, âˆ‡Wâ‚™, pairs, x, sphkernel, Hâ»Â¹)

    GPUCellListSPH.âˆ‚Ïâˆ‚tDDT!(âˆ‘âˆ‚Ïâˆ‚t, âˆ‡Wâ‚™, pairs, x, h, mâ‚€, Î´áµ©, câ‚€, Î³, g, Ïâ‚€, Ï, v, ml) 

    GPUCellListSPH.âˆ‚Î âˆ‚t!(âˆ‘âˆ‚Î âˆ‚t, âˆ‡Wâ‚™, pairs, x, h, Ï, Î±, v, câ‚€, mâ‚€)
    
    GPUCellListSPH.âˆ‚vâˆ‚t!(âˆ‘âˆ‚vâˆ‚t,  âˆ‡Wâ‚™, pairs,  mâ‚€, Ï, câ‚€, Î³, Ïâ‚€) 

    GPUCellListSPH.completed_âˆ‚vâˆ‚t!(âˆ‘âˆ‚vâˆ‚t, âˆ‘âˆ‚Î âˆ‚t,  (0.0, g), gf)

    GPUCellListSPH.update_Ï!(ÏÎ”tÂ½, âˆ‘âˆ‚Ïâˆ‚t, Î”t/2, Ïâ‚€, isboundary)
    
    GPUCellListSPH.update_vpâˆ‚vâˆ‚tÎ”t!(vÎ”tÂ½, âˆ‘âˆ‚vâˆ‚t, Î”t/2, ml) 
 
    GPUCellListSPH.update_xpvÎ”t!(xÎ”tÂ½, vÎ”tÂ½, Î”t/2, ml)

    fill!(âˆ‘âˆ‚Ïâˆ‚t, zero(Float64))
    fill!(âˆ‘âˆ‚Î âˆ‚t, zero(Float64))
    fill!(âˆ‘âˆ‚vâˆ‚t, zero(Float64))

    GPUCellListSPH.âˆ‚Ïâˆ‚tDDT!(âˆ‘âˆ‚Ïâˆ‚t,  âˆ‡Wâ‚™, pairs, xÎ”tÂ½, h, mâ‚€, Î´áµ©, câ‚€, Î³, g, Ïâ‚€, ÏÎ”tÂ½, vÎ”tÂ½, ml) 
    GPUCellListSPH.âˆ‚Î âˆ‚t!(âˆ‘âˆ‚Î âˆ‚t, âˆ‡Wâ‚™, pairs, xÎ”tÂ½, h, ÏÎ”tÂ½, Î±, vÎ”tÂ½, câ‚€, mâ‚€)
    GPUCellListSPH.âˆ‚vâˆ‚t!(âˆ‘âˆ‚vâˆ‚t,  âˆ‡Wâ‚™, pairs,  mâ‚€, ÏÎ”tÂ½, câ‚€, Î³, Ïâ‚€) 

    GPUCellListSPH.completed_âˆ‚vâˆ‚t!(âˆ‘âˆ‚vâˆ‚t, âˆ‘âˆ‚Î âˆ‚t,  (0.0, g), gf)

    GPUCellListSPH.update_all!(Ï, ÏÎ”tÂ½, v, vÎ”tÂ½, x, xÎ”tÂ½, âˆ‘âˆ‚Ïâˆ‚t, âˆ‘âˆ‚vâˆ‚t,  Î”t, Ïâ‚€, isboundary, ml)
    
    etime += Î”t
    Î”t = GPUCellListSPH.Î”t_stepping(buf, âˆ‘âˆ‚vâˆ‚t, v, x, câ‚€, h, CFL)
    end

    #GPUCellListSPH.create_vtp_file(joinpath(path, "./input/OUTPUT.vtk"), x, Ï, âˆ‘âˆ‚vâˆ‚t, v, etime)
end
    #CUDA.registers(@cuda GPUCellListSPH.kernel_âˆ‚Ïâˆ‚tDDT!(âˆ‘âˆ‚Ïâˆ‚t,  âˆ‡Wâ‚™, cellcounter, pairs, gpupoints, h, mâ‚€, Î´áµ©, câ‚€, Î³, g, Ïâ‚€, Ï, v, ml))


sph_simulation(system, sphkernel, Ï, ÏÎ”tÂ½, v, vÎ”tÂ½, xÎ”tÂ½, âˆ‘âˆ‚Î âˆ‚t, âˆ‘âˆ‚Ïâˆ‚t, âˆ‘âˆ‚vâˆ‚t, sumW, sumâˆ‡W, âˆ‡Wâ‚™, Î”t, Ïâ‚€, isboundary, ml, h, Hâ»Â¹, mâ‚€, Î´áµ©, câ‚€, Î³, g, Î±)

@benchmark  sph_simulation($system, $sphkernel, $Ï, $ÏÎ”tÂ½, $v, $vÎ”tÂ½, $xÎ”tÂ½, $âˆ‘âˆ‚Î âˆ‚t, $âˆ‘âˆ‚Ïâˆ‚t, $âˆ‘âˆ‚vâˆ‚t, $sumW, $sumâˆ‡W, $âˆ‡Wâ‚™, $Î”t, $Ïâ‚€, $isboundary, $ml, $h, $Hâ»Â¹, $mâ‚€, $Î´áµ©, $câ‚€, $Î³, $g, $Î±; simn = 100)




using GPUCellListSPH
using CSV, DataFrames, CUDA, BenchmarkTools
using SPHKernels, WriteVTK
path         = dirname(@__FILE__)
fluid_csv    = joinpath(path, "./input/FluidPoints_Dp0.02.csv")
boundary_csv = joinpath(path, "./input/BoundaryPoints_Dp0.02.csv")

cpupoints, DF_FLUID, DF_BOUND    = GPUCellListSPH.loadparticles(fluid_csv, boundary_csv)

dx  = 0.02
h   = 1.2 * sqrt(2) * dx
H   = 2h
hâ»Â¹ = 1/h
Hâ»Â¹ = 1/H
dist = H
Ïâ‚€  = 1000.0
mâ‚€  = Ïâ‚€ * dx * dx #máµ¢  = mâ±¼ = mâ‚€
Î±   = 0.01
g   = 9.81
câ‚€  = sqrt(g * 2) * 20
Î³   = 7
Î”t  = dt  = 1e-5
Î´áµ©  = 0.1
CFL = 0.2
cellsize = (H, H)
sphkernel    = WendlandC2(Float64, 2)

system  = GPUCellListSPH.GPUCellList(cpupoints, cellsize, H)

Ï           = cu(Array([DF_FLUID.Rhop;DF_BOUND.Rhop]))
ml          = cu(append!(ones(Float64, size(DF_FLUID, 1)), zeros(Float64, size(DF_BOUND, 1))))
isboundary  = .!Bool.(ml)
gf          = cu([-ones(size(DF_FLUID,1)) ; ones(size(DF_BOUND,1))])
v           = CUDA.fill((0.0, 0.0), length(cpupoints))


sphprob =  SPHProblem(system, h, H, sphkernel, Ï, v, ml, gf, isboundary, Ïâ‚€, mâ‚€, Î”t, Î±, g, câ‚€, Î³, Î´áµ©, CFL)

stepsolve!(sphprob, 1)


get_points(sphprob)

get_velocity(sphprob)

get_density(sphprob)

get_acceleration(sphprob)


@benchmark stepsolve!($sphprob, 1)

#=
BenchmarkTools.Trial: 946 samples with 1 evaluation.
 Range (min â€¦ max):  4.714 ms â€¦ 42.996 ms  â”Š GC (min â€¦ max): 0.00% â€¦ 54.74%
 Time  (median):     5.193 ms              â”Š GC (median):    0.00%
 Time  (mean Â± Ïƒ):   5.284 ms Â±  1.250 ms  â”Š GC (mean Â± Ïƒ):  0.47% Â±  1.78%

               â–â–ƒâ–„â–„â–ˆâ–…â–†â–„â–…â–‚â–ƒâ–ƒâ–â–
  â–‚â–â–â–‚â–‚â–‚â–ƒâ–„â–„â–„â–„â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–…â–…â–…â–„â–„â–„â–„â–ƒâ–„â–„â–ƒâ–ƒâ–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–‚ â–„
  4.71 ms        Histogram: frequency by time        6.04 ms <

 Memory estimate: 100.20 KiB, allocs estimate: 1938.
=#

#=
findfirst(x-> (x[1] == list[1][1] &&  x[2] == list[1][2]) || (x[2] == list[1][1] &&  x[1] == list[1][2]), Array(pairs))


function Î”t_test(Î±, points, v, câ‚€, h, CFL)
    eta2  = (0.01)h * (0.01)h
    visc  = maximum(@. abs(h * dot(v, points) / (dot(points, points) + eta2)))
    println("visc ", visc)
    dt1   = minimum(@. sqrt(h / norm(Î±)))
    println("dt1 ", dt1)
    dt2   = h / (câ‚€ + visc)
    println("dt2 ",dt2)
    dt    = CFL * min(dt1, dt2)

    return dt
end

Î”t_test(acceleration, points, velocity, câ‚€, h, CFL)

function kernel_Î”t_stepping!(buf, v, points, h, Î·Â²) 
    index = (blockIdx().x - Int32(1)) * blockDim().x + threadIdx().x
    if index <= length(buf)
        vp = v[index]
        pp = points[index]
        buf[index] = abs(h * (vp[1] * pp[1] + vp[2] * pp[2]) / (pp[1]^2 + pp[2]^2 + Î·Â²))
    end
    return nothing
end
function kernel_Î”t_stepping_norm!(buf, a) 
    index = (blockIdx().x - Int32(1)) * blockDim().x + threadIdx().x
    if index <= length(buf)
        buf[index] =  sqrt(a[index, 1]^2 + a[index, 2]^2) 
    end
    return nothing
end
"""    
    Î”t_stepping(buf, a, v, points, câ‚€, h, CFL) 

"""
function Î”t_stepping_test(buf, a, v, points, câ‚€, h, CFL) 
    Î·Â²  = (0.01)h * (0.01)h

    gpukernel = @cuda launch=false kernel_Î”t_stepping_norm!(buf, a) 
    config = launch_configuration(gpukernel.fun)
    Nx = length(buf)
    maxThreads = config.threads
    Tx  = min(maxThreads, Nx)
    Bx = cld(Nx, Tx)
    CUDA.@sync gpukernel(buf, a; threads = Tx, blocks = Bx)

    dt1 = sqrt(h / maximum(buf))
    println("dt1 ", dt1)

    gpukernel = @cuda launch=false kernel_Î”t_stepping!(buf, v, points, h, Î·Â²) 
    config = launch_configuration(gpukernel.fun)
    Nx = length(buf)
    maxThreads = config.threads
    Tx  = min(maxThreads, Nx)
    Bx = cld(Nx, Tx)
    CUDA.@sync gpukernel(buf, v, points, h, Î·Â²; threads = Tx, blocks = Bx)
    
    visc  = maximum(buf)

    println("visc ", visc)

    dt2   = h / (câ‚€ + visc)
    println("dt2 ",dt2)
    dt    = CFL * min(dt1, dt2)

    return dt
end

Î”t_stepping_test(buf, âˆ‘âˆ‚vâˆ‚t, v, x, câ‚€, h, CFL) 
=#
#=

function âˆ‚Î áµ¢â±¼âˆ‚t(list, points, h, Ï, Î±, v, câ‚€, mâ‚€, WgL)
    N    = length(points)

    Î·Â²    = (0.1 * h) * (0.1 * h)

    iter = [1]
    L  = list[1]
        i = L[1]; j = L[2];
        
        Ïáµ¢    = Ï[i]
        Ïâ±¼    = Ï[j]
        váµ¢â±¼   = v[i] - v[j]
        xáµ¢â±¼   = points[i] - points[j]
        Ïáµ¢â±¼   = (Ïáµ¢ + Ïâ±¼) * 0.5

        cond      = dot(váµ¢â±¼, xáµ¢â±¼)

        cond_bool = cond < 0

        Î¼áµ¢â±¼ = h * cond / (dot(xáµ¢â±¼, xáµ¢â±¼) + Î·Â²)
        Î áµ¢â±¼ = cond_bool * (-Î± * câ‚€ * Î¼áµ¢â±¼) / Ïáµ¢â±¼
        
        Î áµ¢â±¼mâ‚€WgLi = Î áµ¢â±¼ * mâ‚€ * WgL[iter]
        
        viscIi   = -Î áµ¢â±¼mâ‚€WgLi
        viscIj   =  Î áµ¢â±¼mâ‚€WgLi

    return viscIi, viscIj
end

=#




#=
function âˆ‚Ïáµ¢âˆ‚tDDTtest(list, points, h, mâ‚€, Î´áµ©, câ‚€, Î³, g, Ïâ‚€, Ï, v, WgL, MotionLimiter)

    Î·Â²   = (0.1*h)*(0.1*h)

    iter = 1
    L    = list[1]
        i = L[1]; j = L[2];

        xáµ¢â±¼   = points[i] - points[j]
        Ïáµ¢    = Ï[i]
        Ïâ±¼    = Ï[j]
        váµ¢â±¼   = v[i] - v[j]
        âˆ‡áµ¢Wáµ¢â±¼ = WgL[iter]

        Cb    = (câ‚€^2 * Ïâ‚€) / Î³

        rÂ²    = dot(xáµ¢â±¼, xáµ¢â±¼)
        println(xáµ¢â±¼)
        println(rÂ²)

        DDTgz = Ïâ‚€ * g / Cb
        DDTkh = 2 * h * Î´áµ©

        dot3  = -dot(xáµ¢â±¼, âˆ‡áµ¢Wáµ¢â±¼)

        println(dot3)

        # Do note that in a lot of papers they write "ij"
        # BUT it should be ji for the direction to match (in dot3)
        # the density direction
        # For particle i
        drz   = xáµ¢â±¼[2]             # 
        rh    = 1 + DDTgz * drz
        
        drhop = Ïâ‚€* ^(rh, 1 / Î³) - Ïâ‚€   # drhop = Ïâ‚€* (rh^invÎ³  - 1)
        println(drhop)
        visc_densi = DDTkh * câ‚€ *(Ïâ±¼ - Ïáµ¢ - drhop) / (rÂ² + Î·Â²)
        println(visc_densi)
        delta_i = visc_densi * dot3 * mâ‚€ / Ïâ±¼
        println(delta_i)

        # For particle j
        drz   = -xáµ¢â±¼[2]
        rh    = 1 + DDTgz * drz
        drhop = Ïâ‚€* ^(rh, 1/Î³) - Ïâ‚€
        visc_densi = DDTkh * câ‚€ * (Ïáµ¢ - Ïâ±¼ - drhop) / (rÂ² + Î·Â²)
        
        delta_j = visc_densi * dot3 * mâ‚€ / Ïáµ¢

        mâ‚€dot     = mâ‚€ * dot(váµ¢â±¼, âˆ‡áµ¢Wáµ¢â±¼) 

        dÏdtIi = mâ‚€dot + delta_i * MotionLimiter[i]
        dÏdtIj = mâ‚€dot + delta_j * MotionLimiter[j]

        dÏdtLi = mâ‚€dot + delta_i * MotionLimiter[i]

    return dÏdtIi, dÏdtIj, dÏdtLi
end

âˆ‚Ïáµ¢âˆ‚tDDTtest(list, points, h, mâ‚€, Î´áµ©, câ‚€, Î³, g, Ïâ‚€, density, velocity, WgL, MotionLimiter)


âˆ‡Wâ‚™[18631]
WgL[1]

function kernel_âˆ‚Ïâˆ‚tDDT_test!(âˆ‘âˆ‚Ïâˆ‚t,  âˆ‡Wâ‚™, pairs, points, h, mâ‚€, Î´áµ©, câ‚€, Î³, g, Ïâ‚€, Ï, v, MotionLimiter) 
    index = 18631
    if index <= length(pairs)
        pair  = pairs[index]
        páµ¢    = pair[1]; pâ±¼ = pair[2]; d = pair[3]
        if !isnan(d)

            Î³â»Â¹  = 1/Î³
            Î·Â²   = (0.1*h)*(0.1*h)
            Cb    = (câ‚€ * câ‚€ * Ïâ‚€) * Î³â»Â¹
            DDTgz = Ïâ‚€ * g / Cb
            DDTkh = 2 * h * Î´áµ©
    
            #=
            Cb = (câ‚€ * câ‚€ * Ïâ‚€) * Î³â»Â¹
            Pá´´ =  Ïâ‚€ * g * z
            áµ¸áµ€á´´
            =#

            xáµ¢    = points[páµ¢]
            xâ±¼    = points[pâ±¼]
            Ïáµ¢    = Ï[páµ¢]
            Ïâ±¼    = Ï[pâ±¼]

            
            Î”x    = (xáµ¢[1] - xâ±¼[1], xáµ¢[2] - xâ±¼[2])
            Î”v    = (v[páµ¢][1] - v[pâ±¼][1], v[páµ¢][2] - v[pâ±¼][2])

            âˆ‡Wáµ¢   = âˆ‡Wâ‚™[index]

            #rÂ²    = (xáµ¢[1]-xâ±¼[1])^2 + (xáµ¢[2]-xâ±¼[2])^2
            rÂ² = d^2  #  xáµ¢â‹… xâ±¼ = d^2
            println(rÂ²)
            #=
            z  = Î”x[2]
            Cb = (câ‚€ * câ‚€ * Ïâ‚€) * Î³â»Â¹
            Pá´´ =  Ïâ‚€ * g * z
            Ïá´´ =  Ïâ‚€ * (((Pá´´ + 1)/Cb)^Î³â»Â¹ - 1)
            Ïˆ  = 2 * (Ïáµ¢ - Ïâ±¼) * Î”x / rÂ²
            =#
            
            dot3  = -(Î”x[1] * âˆ‡Wáµ¢[1] + Î”x[2] * âˆ‡Wáµ¢[2]) #  - Î”x â‹… âˆ‡Wáµ¢ 
            println(dot3)
            
            drhopvp = Ïâ‚€ * (1 + DDTgz * Î”x[2])^Î³â»Â¹ - Ïâ‚€
            
            visc_densi = DDTkh * câ‚€ * (Ïâ±¼ - Ïáµ¢ - drhopvp) / (rÂ² + Î·Â²)
            
            delta_i    = visc_densi * dot3 * mâ‚€ / Ïâ±¼

            drhopvn = Ïâ‚€ * (1 - DDTgz * Î”x[2])^Î³â»Â¹ - Ïâ‚€
            println(drhopvn)

            visc_densi = DDTkh * câ‚€ * (Ïáµ¢ - Ïâ±¼ - drhopvn) / (rÂ² + Î·Â²)
            println("DDTkh =" , DDTkh, "; visc_densi =", visc_densi)
            delta_j    = visc_densi * dot3 * mâ‚€ / Ïáµ¢
            println(delta_i)
            mâ‚€dot     = mâ‚€ * (Î”v[1] * âˆ‡Wáµ¢[1] + Î”v[2] * âˆ‡Wáµ¢[2])  #  Î”v â‹… âˆ‡Wáµ¢

            âˆ‘âˆ‚Ïâˆ‚ti = (mâ‚€dot + delta_i * MotionLimiter[páµ¢])
            âˆ‘âˆ‚Ïâˆ‚tj = (mâ‚€dot + delta_j * MotionLimiter[pâ±¼])
            
        end
    end
    âˆ‘âˆ‚Ïâˆ‚ti, âˆ‘âˆ‚Ïâˆ‚tj
end

kernel_âˆ‚Ïâˆ‚tDDT_test!(âˆ‘âˆ‚Ïâˆ‚t,  âˆ‡Wâ‚™, pairs, points, h, mâ‚€, Î´áµ©, câ‚€, Î³, g, Ïâ‚€, Ï, v, MotionLimiter) 
=#


#=
        using CUDA

        function test!(x) 
            gpukernel = @cuda launch=false kernel_test!(x) 
            config = launch_configuration(gpukernel.fun)
            Nx = length(x)
            maxThreads = config.threads
            #maxBlocks  = config.blocks
            Tx  = min(maxThreads, Nx)
            #Bx  = min(maxBlocks, cld(Nx, Tx))
            Bx  = cld(Nx, Tx)
            CUDA.@sync gpukernel(x; threads = Tx, blocks = Bx)
        end
        function kernel_test!(x) 
            index  = (blockIdx().x - Int32(1)) * blockDim().x + threadIdx().x
            stride = gridDim().x * blockDim().x
            i = index
            n = 1
            while  i <= length(x)
                x[i] = n
                i += stride
                n += 1
            end
            return nothing
        end
        x = CUDA.zeros(1000000000)
        test!(x) 
        sum(x)


        function test2!(x) 
            gpukernel = @cuda launch=false kernel_test2!(x) 
            config = launch_configuration(gpukernel.fun)
            Nx = length(x)
            maxThreads = config.threads
            maxThreads = 3
            Tx  = min(maxThreads, Nx)
            CUDA.@sync gpukernel(x; threads = Tx, blocks = 1)
        end
        function kernel_test2!(x) 
            index  = threadIdx().x
            stride = blockDim().x
            i = index
            while i <= length(x)
                @cuprintln "i = $i, index = $index, threadIdx: $(threadIdx().x), blockIdx $(blockIdx().x), blockDim $(blockDim().x)" 
                i += stride
            end
            return nothing
        end
        test2!(CUDA.zeros(10)) 

        @benchmark minmax(34, 23)


using CUDA

        function test_2d!(cnt, mat)
            gpukernel = @cuda launch=false kernel_test_2d!(cnt, mat, 6)
            config = launch_configuration(gpukernel.fun)
            maxThreads = config.threads
            Nx = length(mat)
            Tx  = min(maxThreads, Nx) 
            Bx  = 1
            cs = fld(attribute(device(), CUDA.DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK_OPTIN) - 8,  Tx * sizeof(Tuple{Int32, Int32}))
            CUDA.@sync gpukernel(cnt, mat, cs; threads = Tx, blocks = Bx, shmem= Tx * cs * sizeof(Tuple{Int32, Int32}))
        end
        function kernel_test_2d!(cnt, mat, cs)
            index  = (blockIdx().x - Int32(1)) * blockDim().x + threadIdx().x
            stride = gridDim().x * blockDim().x
            scnt   = CuStaticSharedArray(Int32, 1)
     
            if threadIdx().x == 1
                scnt[1] = cnt[1] 
            end
            sync_threads()
            Nx, Ny = size(mat)
            while index <= length(mat)
                y = cld(index, Nx)
                x = index - Nx * (y - 1)
                #@cuprintln "index $index ind $x, $y"

                n = CUDA.@atomic scnt[1] += 1
                mat[x,y] = n + 1
                index += stride
            end
            sync_threads()
            if threadIdx().x == 1
                cnt[1] = scnt[1] 
                CUDA.@cuprintln "Down $(cnt[1])  block $(blockIdx().x)"
            end
            return nothing
        end
        cnt = cu([0])
        mat = CUDA.zeros(100, 100)
        test_2d!(cnt, mat)



        function pranges_test!(ranges, pairs) 
            gpukernel = @cuda launch=false kernel_pranges_test!(ranges, pairs) 
            config = launch_configuration(gpukernel.fun)
            Nx = length(ranges)
            maxThreads = config.threads
            Tx  = min(maxThreads, Nx)
            CUDA.@sync gpukernel(ranges, pairs; threads = 1, blocks = 1, shmem = Tx * sizeof(Int))
        end
        function kernel_pranges_test!(ranges, pairs, np) 
            index  = (blockIdx().x - Int32(1)) * blockDim().x + threadIdx().x
            thread = threadIdx().x
            stride = gridDim().x * blockDim().x
            tn     = blockDim().x
            cache  = CuDynamicSharedArray(Int, tn)
            cache[thread] = 1

            si = (thread - 1) * np + 1
            ei = min(length(pairs), thread * np)
            sync_threads()
            if thread != 1
                s = first(pairs[si])
                for i = si+1:length(pairs)
                    if first(pairs[i]) != s
                        si = i
                        break
                    end
                end
            end 
      
            sync_threads()            
            for i = si:ei
        
            end
        end


        @benchmark  pranges_test!($pr, $system.pairs)

=#
#=
function kernel_neiblist_2d!(nlist, ncnt, points,  celllist, cellpnum, pcell, dist, offset) 
    index = (blockIdx().x - Int32(1)) * blockDim().x + threadIdx().x
    if index <= length(points)
        # get point cell
        cell   = pcell[index]
        celli  = cell[1] + offset[1]
        cellj  = cell[2] + offset[2]
        if  0 < celli <= size(celllist, 2) && 0 < cellj <= size(celllist, 3)
            clist  = view(celllist, :, celli, cellj)
            celln  = cellpnum[celli, cellj]
            distsq = dist^2
            cnt    = ncnt[index]
            for i = 1:celln
                indexj = clist[i]
                if index != indexj && (points[index][1] - points[indexj][1])^2 + (points[index][2] - points[indexj][2])^2 < distsq
                    cnt += 1
                    if cnt <= size(nlist, 1)
                        nlist[cnt, index] = indexj
                    end
                end
            end
            ncnt[index] = cnt
        end
    end
    return nothing
end
"""
    neiblist_2d!(nlist, ncnt, points,  celllist, cellpnum, pcell, dist, offset)

"""
function neiblist_2d!(nlist, ncnt, points,  celllist, cellpnum, pcell, dist, offset)
    gpukernel = @cuda launch=false kernel_neiblist_2d!(nlist, ncnt, points,  celllist, cellpnum, pcell, dist, offset)
    config = launch_configuration(gpukernel.fun)
    Nx = length(points)
    maxThreads = config.threads
    Tx  = min(maxThreads, Nx)
    Bx  = cld(Nx, Tx)
    CUDA.@sync gpukernel(nlist, ncnt, points,  celllist, cellpnum, pcell, dist, offset; threads = Tx, blocks = Bx)
end
=#
#=
function âˆ‚Ïâˆ‚tDDT2!(âˆ‘âˆ‚Ïâˆ‚t,  âˆ‡W, pairs, points, ranges, h, mâ‚€, Î´áµ©, câ‚€, Î³, g, Ïâ‚€, Ï, v, ptype) 
    if length(pairs) != length(âˆ‡W) error("Length shoul be equal") end

    gpukernel = @cuda launch=false kernel_âˆ‚Ïâˆ‚tDDT2!(âˆ‘âˆ‚Ïâˆ‚t,  âˆ‡W, pairs, points, ranges, h, mâ‚€, Î´áµ©, câ‚€, Î³, g, Ïâ‚€, Ï, v, ptype) 
    config = launch_configuration(gpukernel.fun)
    Nx = length(ranges)
    maxThreads = config.threads
    Tx  = min(maxThreads, Nx)
    Bx  = cld(Nx, Tx)
    CUDA.@sync gpukernel(âˆ‘âˆ‚Ïâˆ‚t, âˆ‡W, pairs, points, ranges, h, mâ‚€, Î´áµ©, câ‚€, Î³, g, Ïâ‚€, Ï, v, ptype; threads = Tx, blocks = Bx)
end
function kernel_âˆ‚Ïâˆ‚tDDT2!(âˆ‘âˆ‚Ïâˆ‚t,  âˆ‡W, pairs, points, ranges, h, mâ‚€, Î´áµ©, câ‚€, Î³, g, Ïâ‚€, Ï, v, ptype) 
    index  = (blockIdx().x - Int32(1)) * blockDim().x + threadIdx().x
    stride = gridDim().x * blockDim().x
    #s      = index * (stride - 1) + index
    #e      = stride - 1
    # move it outside kernel
    Î³â»Â¹  = 1/Î³
    Î·Â²   = (0.1*h)*(0.1*h)
    Cb    = (câ‚€ * câ‚€ * Ïâ‚€) * Î³â»Â¹
    DDTgz = Ïâ‚€ * g / Cb
    DDTkh = 2 * h * Î´áµ©

    while index <= length(ranges)
        s, e = ranges[index]
        for pind in s:e
        pair  = pairs[index]
        páµ¢    = pair[1]; pâ±¼ = pair[2]
        if páµ¢ > 0 # && !(isboundary[páµ¢] && isboundary[páµ¢]) 
            xáµ¢    = points[páµ¢]
            xâ±¼    = points[pâ±¼]
            Î”x    = (xáµ¢[1] - xâ±¼[1], xáµ¢[2] - xâ±¼[2])
            rÂ²    = Î”x[1]^2 + Î”x[2]^2 
            # for timestep Î”tÂ½ d != actual range
            # one way - not calculate values out of 2h
            # if rÂ² > (2h)^2 return nothing end
            #=
            Cb = (câ‚€ * câ‚€ * Ïâ‚€) * Î³â»Â¹
            Pá´´ =  Ïâ‚€ * g * z
            áµ¸áµ€á´´
            =#
            Ïáµ¢    = Ï[páµ¢]
            Ïâ±¼    = Ï[pâ±¼]

            Î”v    = (v[páµ¢][1] - v[pâ±¼][1], v[páµ¢][2] - v[pâ±¼][2])

            âˆ‡Wáµ¢â±¼  = âˆ‡W[index]
            #=
            z  = Î”x[2]
            Cb = (câ‚€ * câ‚€ * Ïâ‚€) * Î³â»Â¹
            Pá´´ =  Ïâ‚€ * g * z
            Ïá´´ =  Ïâ‚€ * (((Pá´´ + 1)/Cb)^Î³â»Â¹ - 1)
            Ïˆ  = 2 * (Ïáµ¢ - Ïâ±¼) * Î”x / rÂ²
            =#
            dot3  = -(Î”x[1] * âˆ‡Wáµ¢â±¼[1] + Î”x[2] * âˆ‡Wáµ¢â±¼[2]) #  - Î”x â‹… âˆ‡Wáµ¢â±¼

            # as actual range at timestep Î”tÂ½  may be greateg  - some problems can be here
            if 1 + DDTgz * Î”x[2] < 0 || 1 - DDTgz * Î”x[2] < 0 return nothing end
            
            mâ‚€dot     = mâ‚€ * (Î”v[1] * âˆ‡Wáµ¢â±¼[1] + Î”v[2] * âˆ‡Wáµ¢â±¼[2])  #  Î”v â‹… âˆ‡Wáµ¢â±¼
            âˆ‘âˆ‚Ïâˆ‚ti = âˆ‘âˆ‚Ïâˆ‚tj = mâ‚€dot

            if ptype[páµ¢] >= 1
                drhopvp = Ïâ‚€ * powfancy7th(1 + DDTgz * Î”x[2], Î³â»Â¹, Î³) - Ïâ‚€ ## << CHECK
                visc_densi = DDTkh * câ‚€ * (Ïâ±¼ - Ïáµ¢ - drhopvp) / (rÂ² + Î·Â²)
                delta_i    = visc_densi * dot3 * mâ‚€ / Ïâ±¼
                âˆ‘âˆ‚Ïâˆ‚ti    += delta_i 
            end
            CUDA.@atomic âˆ‘âˆ‚Ïâˆ‚t[páµ¢] += âˆ‘âˆ‚Ïâˆ‚ti 

            if ptype[pâ±¼] >= 1
                drhopvn = Ïâ‚€ * powfancy7th(1 - DDTgz * Î”x[2], Î³â»Â¹, Î³) - Ïâ‚€
                visc_densi = DDTkh * câ‚€ * (Ïáµ¢ - Ïâ±¼ - drhopvn) / (rÂ² + Î·Â²)
                delta_j    = visc_densi * dot3 * mâ‚€ / Ïáµ¢
                âˆ‘âˆ‚Ïâˆ‚tj    += delta_j 
            end
            CUDA.@atomic âˆ‘âˆ‚Ïâˆ‚t[pâ±¼] += âˆ‘âˆ‚Ïâˆ‚tj
            
            #=
            if isnan(delta_j) || isnan(mâ‚€dot)  || isnan(Ïáµ¢) || isnan(Ïâ±¼) 
                @cuprintln "kernel_DDT 1 isnan dx1 = $(Î”x[1]) , dx2 = $(Î”x[2]) rhoi = $Ïáµ¢ , dot3 = $dot3 , visc_densi = $visc_densi drhopvn = $drhopvn $(âˆ‡W[1]) $(Î”v[1])"
                error() 
            end
            if isinf(delta_j) || isinf(mâ‚€dot)  || isinf(delta_i) 
                @cuprintln "kernel_DDT 2 inf: dx1 = $(Î”x[1]) , dx2 = $(Î”x[2]) rhoi = $Ïáµ¢ , rhoj = $Ïâ±¼ , dot3 = $dot3 ,  delta_i = $delta_i , delta_j = $delta_j , drhopvn = $drhopvn , visc_densi = $visc_densi , $(âˆ‡W[1]) , $(Î”v[1])"
                error() 
            end
            =#
            #mlfac = MotionLimiter[páµ¢] * MotionLimiter[pâ±¼]
            #=
            if isnan(âˆ‘âˆ‚Ïâˆ‚tval1) || isnan(âˆ‘âˆ‚Ïâˆ‚tval2) || abs(âˆ‘âˆ‚Ïâˆ‚tval1) >  10000000 || abs(âˆ‘âˆ‚Ïâˆ‚tval2) >  10000000
                @cuprintln "kernel DDT: drhodti = $âˆ‘âˆ‚Ïâˆ‚ti drhodtj = $âˆ‘âˆ‚Ïâˆ‚tj, dx1 = $(Î”x[1]), dx2 = $(Î”x[2]) rhoi = $Ïáµ¢, rhoj = $Ïâ±¼, dot3 = $dot3, visc_densi = $visc_densi, drhopvn = $drhopvn, dw = $(âˆ‡W[1]),  dv = $(Î”v[1])"
                error() 
            end
            =#
            
        end
        index += stride
        end
    end
    return nothing
end
=#


struct MyStr{T}
    Î”xË£::T
    Î”xÊ¸::T
    Î”vË£::T
    Î”vÊ¸::T
    Ïáµ¢::T
    Ïâ±¼::T
end

function pairs_calk!(buff, pairs; minthreads::Int = 1024) 
    gpukernel = @cuda launch=false kernel_pairs_calk!(buff, pairs) 
    config = launch_configuration(gpukernel.fun)
    Nx = length(pairs)
    maxThreads = config.threads
    Tx  = min(minthreads, maxThreads, Nx)
    Bx  = cld(Nx, Tx)
    CUDA.@sync gpukernel(buff, pairs; threads = Tx, blocks = Bx)
end
function kernel_pairs_calk!(buff, pairs) 
    index = (blockIdx().x - Int32(1)) * blockDim().x + threadIdx().x
    if index <= length(pairs)
        buff[1][index] = 1.2
        buff[2][index] = 4.5
        buff[3][index] = 0.1
        buff[4][index] = 0.7
        buff[5][index] = 0.7
        buff[6][index] = 0.7
        buff[7][index] = 0.7
        buff[8][index] = 0.7
    end
    return nothing
end
function pairs_calk2!(buff, pairs; minthreads::Int = 1024)  
    gpukernel = @cuda launch=false maxregs=64 kernel_pairs_calk2!(buff, pairs) 
    config = launch_configuration(gpukernel.fun)
    Nx = length(pairs)
    maxThreads = config.threads
    Tx  = min(minthreads, maxThreads, Nx)
    Bx  = cld(Nx, Tx)
    CUDA.@sync gpukernel(buff, pairs; threads = Tx, blocks = Bx)
end
function kernel_pairs_calk2!(buff::AbstractArray{MyStr{T}}, pairs)  where T
    index = (blockIdx().x - Int32(1)) * blockDim().x + threadIdx().x
    if index <= length(buff)
        buff[index] = MyStr{T}(0.1, 0.2, 0.3, 0.4, 0.9, 1.2)
    end
    return nothing
end

FT = Float32
PN = 10000
buff = (CUDA.zeros(FT, PN), CUDA.zeros(FT, PN), CUDA.zeros(FT, PN), CUDA.zeros(FT, PN), CUDA.zeros(FT, PN), CUDA.zeros(FT, PN), CUDA.zeros(FT, PN), CUDA.zeros(FT, PN))

prs = CUDA.zeros(PN )
@benchmark pairs_calk!($buff,$prs ; minthreads = 1024)

buff2 = CuArray{MyStr{FT}}(undef, PN)

@benchmark pairs_calk2!($buff2,$pairs ; minthreads = 1024)

zv = @SVector zeros(8)
buff3 = CUDA.fill(zv, PN)
function pairs_calk3!(buff, pairs; minthreads::Int = 1024) 
    gpukernel = @cuda launch=false kernel_pairs_calk3!(buff, pairs) 
    config = launch_configuration(gpukernel.fun)
    Nx = length(pairs)
    maxThreads = config.threads
    Tx  = min(minthreads, maxThreads, Nx)
    Bx  = cld(Nx, Tx)
    CUDA.@sync gpukernel(buff, pairs; threads = Tx, blocks = Bx)
end
function kernel_pairs_calk3!(buff, pairs) 
    index = (blockIdx().x - Int32(1)) * blockDim().x + threadIdx().x
    if index <= length(pairs)
        buff[index] = SVector((0.1, 0.2, 0.3, 0.4, 0.9, 1.2, 4.5, 7.8))
    end
    return nothing
end
@benchmark pairs_calk3!($buff3,$prs ; minthreads = 1024)